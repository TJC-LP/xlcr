# Implementation Plan for CLI-based Spreadsheet Compression Tool

## Architecture Overview
The tool will be structured into modular components that mirror the SheetCompressor pipeline proposed by Microsoft ([Microsoft unveils software that allows LLMs to work with spreadsheets](https://techxplore.com/news/2024-07-microsoft-unveils-software-llms-spreadsheets.html#:~:text=To%20implement%20SheetCompressor%2C%20the%20researchers,understand%20what%20the%20spreadsheet%20does)). Key components include:

- **Spreadsheet Parser (Input Module)** – Uses Apache POI (or similar) to load Excel files in various formats (.xlsx via XSSFWorkbook, .xls via HSSFWorkbook, .xlsm similarly to .xlsx, and .xlsb via POI’s XSSFBReader streaming API ([XLSBUnsupportedException (POI API Documentation) - Apache POI](https://poi.apache.org/apidocs/dev/org/apache/poi/xssf/XLSBUnsupportedException.html#:~:text=We%20don%27t%20support%20,of%20xlsb%20files%20via%20XSSFBReader))). This module abstracts format differences and produces a unified in-memory representation of the workbook.
- **In-Memory Data Model** – Represents the spreadsheet content in Scala objects (e.g. case classes for Workbook, Sheet, Cell). Each Cell will store its address (e.g. "B3"), raw value (text, number, formula, etc.), and formatting info (style flags, number format string, etc.). This model provides the input for compression algorithms.
- **Compression Engine** – A set of deterministic transformation modules applied in sequence:
    - *Structural Anchor Extractor* – Identifies structural anchors (key rows/columns) and prunes away less-informative cells ([Microsoft's SpreadsheetLLM: Revolutionising Spreadsheet Analysis with AI](https://www.ryanmcdonough.co.uk/microsofts-spreadsheetllm-revolutionising-spreadsheet-analysis-with-ai/#:~:text=1.%20Structural,component%20recognises%20patterns%20in%20data)) ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=3.2%20Structural,the%20edges%20of%20table%20bound)). It produces a **skeleton** of the sheet focusing on heterogeneous edges while discarding large homogeneous regions.
    - *Inverted-Index Translator* – Converts the skeleton grid into a compact dictionary format ([Microsoft unveils software that allows LLMs to work with spreadsheets](https://techxplore.com/news/2024-07-microsoft-unveils-software-llms-spreadsheets.html#:~:text=Once%20in%20place%2C%20rows%20and,for%20data%20%2034%20aggregation)). It maps unique cell content to locations, merging duplicates and omitting empties ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=To%20address%20these%20inefficiencies%2C%20we,merged%2C%20with%20empty%20cells%20excluded)).
    - *Data-Format Aggregator* – Analyzes cell data types and merges contiguous cells with similar formats into aggregated regions ([Microsoft's SpreadsheetLLM: Revolutionising Spreadsheet Analysis with AI](https://www.ryanmcdonough.co.uk/microsofts-spreadsheetllm-revolutionising-spreadsheet-analysis-with-ai/#:~:text=reduces%20token%20usage%2C%20especially%20for,without%20losing%20essential%20structural%20information)). It replaces detailed values (especially large numeric blocks) with generalized tokens or format descriptors to further reduce output size ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=values%20are%20not%20essential%20for,rules%20to%20match%20the%20value)).
- **JSON Output Generator** – Traverses the compressed representation and produces a structured JSON output. This includes sheet structure, cell content mappings, and uses markdown-like syntax in strings to preserve formatting (e.g. bold text **bold**, formulas in backticks, links in `[text](url)` format). We will use a Scala JSON library (such as Circe or Jackson) to ensure correct JSON encoding.
- **CLI Interface** – A command-line front-end (built perhaps with a Scala CLI parser like scopt) that ties everything together. It handles user arguments (file paths, options) and orchestrates the pipeline: reading input, invoking the Compression Engine, and writing out the JSON. The CLI will manage multithreading (if enabled) and ensure the tool operates as a local offline program.

These components interact sequentially: the Parser loads data into the Model, the Compression Engine modules transform the Model step by step, and finally the JSON Generator produces output. This modular design makes it easy to maintain or extend each part (for example, swapping out the parser for a different library, or adding new compression rules) without affecting other components.

## Processing Pipeline
The execution flow proceeds in clear stages from loading a spreadsheet to producing the compressed JSON output. Below is the step-by-step pipeline, which the CLI tool will execute for each input file:

1. **Load Spreadsheet**: The CLI accepts an input file path (or multiple). The parser module determines the format and uses the appropriate POI API to load it. For example, `.xlsx`/`.xlsm` files use XSSFWorkbook, `.xls` uses HSSFWorkbook, and `.xlsb` (Excel binary) is handled via POI’s XSSFBReader event model (since XSSFWorkbook doesn’t directly open .xlsb) ([XLSBUnsupportedException (POI API Documentation) - Apache POI](https://poi.apache.org/apidocs/dev/org/apache/poi/xssf/XLSBUnsupportedException.html#:~:text=We%20don%27t%20support%20,of%20xlsb%20files%20via%20XSSFBReader)). The file is read into an internal **Workbook** object containing one or more sheets.

2. **Initialize Data Model**: For each **Sheet**, iterate through rows and cells to populate the data model. Each cell entry will include: the cell address (e.g. "Sheet1!A1"), the value (as string or numeric type), any formula expression, and cell formatting (number format code, text style like bold/italic, fill color, etc.). This step provides the raw “vanilla” serialization of the sheet (addresses, values, formats) as a baseline representation ([SpreadsheetLLM: Encoding Spreadsheets for Large Language Models - Microsoft Research](https://www.microsoft.com/en-us/research/publication/encoding-spreadsheets-for-large-language-models/#:~:text=challenges%20for%20large%20language%20models,It%20significantly%20improves%20performance%20in)). If memory is a concern for very large files, this could be done in a streaming fashion (row by row) rather than loading everything at once.

3. **Structural-Anchor Extraction**: Compress the spreadsheet by removing low-information bulk while preserving layout-defining elements ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=3.2%20Structural,the%20edges%20of%20table%20bound)). This involves:
    - **Identifying Anchors**: Scan the sheet for **heterogeneous** rows and columns that likely define table boundaries ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=3.2%20Structural,the%20edges%20of%20table%20bound)). Heuristics from the research will be applied: find “discrepancies in neighboring rows and columns” in terms of content types, formatting or emptiness ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=Initially%2C%20this%20method%20enumerates%20bounding,layout%20understanding%20of%20a%20spread)). For example, a row that switches from text to numbers (or has a mix of text/number where adjacent rows are uniform) might indicate a header or summary row (anchor). Similarly, a column with text identifiers next to columns of numbers indicates a structural boundary on the side. We also consider formatting cues (merged cells, bold headers, distinct fill colors) to identify table headers or section titles ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=Initially%2C%20this%20method%20enumerates%20bounding,layout%20understanding%20of%20a%20spread)).
    - **Pruning**: Once anchor rows (`r_p`) and columns (`c_q`) at table edges are identified, remove rows and columns that are more than *k* cells away from any anchor ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=Using%20these%20anchor%20points%2C%20our,method%20discards)). The parameter *k* (configurable, e.g. default 1) controls how many neighboring rows/cols around each anchor are retained as context. Everything beyond that distance is dropped because such interior regions of purely homogeneous data “rarely serve as table boundaries” ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=Using%20these%20anchor%20points%2C%20our,method%20discards)). For instance, in a sheet with 500 rows of similar data, if row 1 is a header anchor, we might keep row 2 (k=1 neighbor) but discard a bulk of middle rows until perhaps the last summary anchor. The sheet is essentially **skeletonized** – most content removed except the structural frame (important top/bottom rows and left/right columns) ([Microsoft unveils software that allows LLMs to work with spreadsheets](https://techxplore.com/news/2024-07-microsoft-unveils-software-llms-spreadsheets.html#:~:text=Once%20in%20place%2C%20rows%20and,for%20data%20%2034%20aggregation)). In practice, this step can filter out a large portion of cells (the paper reports ~75% content filtered) while preserving ~97% of the key rows/columns that define the layout ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=Furthermore%2C%20after%20extraction%2C%20we%20perform,the%20edges%20of%20table%20boundaries)).
    - **Coordinate Remapping**: After pruning, adjust the cell coordinates to close gaps ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=Furthermore%2C%20after%20extraction%2C%20we%20perform,the%20edges%20of%20table%20boundaries)). For example, if rows 5–500 were removed, the sheet’s remaining content might jump from row 4 to row 501 originally. We renumber these so that row 501’s content becomes the new row 5 in the compressed sheet. This **re-indexing** maintains the relative layout in a contiguous range, ensuring that formulas or references (if any) and the overall table structure remain consistent in the compressed view ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=Furthermore%2C%20after%20extraction%2C%20we%20perform,the%20edges%20of%20table%20boundaries)). The output of this stage is a reduced sheet model `S_e` containing only the anchor regions (a skeletal structure).

4. **Inverted-Index Translation**: Convert the pruned sheet data into a dictionary-like representation to eliminate redundancy ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=To%20address%20these%20inefficiencies%2C%20we,merged%2C%20with%20empty%20cells%20excluded)). This stage operates on the skeleton `S_e`:
    - **Dictionary Construction**: Iterate over each remaining cell in the sheet and insert its content into a map. Each unique cell **value** (text or numeric) becomes a key, and its location(s) become the value. We do *not* create entries for truly empty cells, as they carry no information and would waste space ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=columns%2C%20and%20scattered%20cells,ther%20exacerbating%20token%20usage)). As a result, the two-dimensional grid is flattened into a set of key-value pairs, where the key is a cell’s content and the value is one or more cell addresses where that content appears ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=To%20address%20these%20inefficiencies%2C%20we,merged%2C%20with%20empty%20cells%20excluded)). For example, if "USA" appears in cells A2, A5, A8, the dictionary might have `"USA": ["A2","A5","A8"]`. This departure from row-by-row listing saves tokens by avoiding repetition of identical values and by not listing every empty cell ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=columns%2C%20and%20scattered%20cells,ther%20exacerbating%20token%20usage)).
    - **Merging Address Ranges**: To further compress, merge consecutive cell addresses into ranges when possible ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=traditional%20matrix,merged%2C%20with%20empty%20cells%20excluded)). If a value appears in a continuous sequence of cells, we represent that as a single range (e.g. `"0": "B2:B100"` instead of 99 separate entries for zeros in B2 through B100). Ranges can be determined row-wise or column-wise; for simplicity, we will merge linear sequences in the same row or column. The dictionary thus maps each unique content to either a single address or a compact address range ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=traditional%20matrix,merged%2C%20with%20empty%20cells%20excluded)). This **lossless inverted index** preserves all information but in a highly compact form ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=depart%20from%20traditional%20row,usage%20while%20preserving%20data%20integrity)). By using JSON to encode this map of values to addresses, we get a structure that is both machine-readable and easy for an LLM to parse. (In the paper’s context, this step improved compression dramatically, boosting the compression ratio from ~4× to ~15× ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=Inverted,be%20found%20in%20Table%201)).)

5. **Data-Format-Aware Aggregation**: Apply format-based rules to compress regions of similar data, focusing especially on numeric or date-heavy areas ([Microsoft's SpreadsheetLLM: Revolutionising Spreadsheet Analysis with AI](https://www.ryanmcdonough.co.uk/microsofts-spreadsheetllm-revolutionising-spreadsheet-analysis-with-ai/#:~:text=reduces%20token%20usage%2C%20especially%20for,without%20losing%20essential%20structural%20information)) ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=values%20are%20not%20essential%20for,rules%20to%20match%20the%20value)). Even after the previous steps, a sheet might have many unique numeric values that are structurally similar (e.g. a column of revenue figures). This module reduces such sequences by abstracting their values:
    - **Type Detection**: Examine each cell’s value (and its Excel number format, if any) to determine its **data type** ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=critical%20for%20understanding%20spreadsheets,reducing%20the%20number%20of%20tokens)) ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=ever%2C%20spreadsheet%20users%20do%20not,world%20corpora)). We will use a combination of the cell’s Number Format String (NFS) and content pattern matching. For example: if a cell’s NFS is `"yyyy-mm-dd"` or the value matches a date pattern, classify as Date; if the format or value has a "%" sign, classify as Percentage; if it’s a pure number with two decimal places, maybe Currency or Float depending on currency symbol; if it contains "@" symbol, classify as Email, etc. We’ll implement a rule-based mapper covering types mentioned in the research: Year, Integer, Float, Percentage, ScientificNotation, Date, Time, Currency, Email, and Others ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=ever%2C%20spreadsheet%20users%20do%20not,world%20corpora)). (We expect these cover a majority of cases ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=ever%2C%20spreadsheet%20users%20do%20not,world%20corpora)).) If a cell has a custom NFS, we use that format as a descriptor; if no NFS, fall back to our rules on the cell’s text. Each cell in the skeleton now gets an associated type label (either one of the known types or "Others").
    - **Clustering Adjacent Cells**: Group contiguous cells of the **same type** into aggregate regions ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=3,format%20strings%20and%20data%20types)). We perform a flood-fill (DFS/BFS) on the sheet grid using the type labels. For each unvisited cell, if it’s of type X, we mark all connected neighbors (up, down, left, right) of the same type X as part of one cluster. We then record the bounding box of that cluster (top-left and bottom-right cell coordinates) along with the type X. For example, a block of date values in a column might form one region from A2:A50 of type "Date". This step effectively identifies large blocks of similar data. (The algorithm from the paper’s appendix will guide this grouping ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=Algorithm%201%3A%20Identical%20Cell%20Aggregation,nfs%20composed%20of%20all%20cell)) ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=5%20Function%20dfs,c%5D%29%20%E2%88%A8%20val_type%21)), ensuring we only combine truly adjacent cells of identical type.)
    - **Aggregate Representation**: For each homogeneous region found, replace the multitude of individual values with a single representative descriptor. We will use **markdown-like tokens or format strings** to denote the region’s content: e.g., instead of listing every date in a Date region, we might use the format code `"yyyy-mm-dd"` or a placeholder text like "`<Date>`" to represent all those cells. Similarly, a large numeric region could be represented by "`<Number>`" with perhaps an indication of format (if they all share one, e.g. "`#,##0.00`"). The JSON output will include these as keys in place of actual values. This sacrifices fine detail (exact numbers) for brevity, but crucially retains the **semantic structure** – we know that there is a block of numbers and what their general format or type is ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=values%20are%20not%20essential%20for,rules%20to%20match%20the%20value)). (Losing the precise values "18,476" vs "18,674" does not hinder understanding that "this column represents revenue figures" ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=In%20spreadsheets%2C%20adjacent%20cells%20typically,contrast%2C%20the%20data%20type%20is)).) We ensure that textual or key values (like column headers or unique IDs) are *not* aggregated away by this step – those remain as-is so that important labels and distinct entries stay intact. After aggregation, the compression ratio improves even further (the paper achieved ~25× total compression) ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=where%20R%20denotes%20the%20predefined,are%20displayed%20in%20Table%201)).

6. **JSON Assembly**: With all compressions applied, generate the final JSON structure. The output JSON will capture sheet structure, cell content, and formatting cues in a structured way:
    - **Sheet Structure**: If the workbook has multiple sheets, the JSON can be an object with a top-level `"sheets"` array. Each entry contains a `"name"` for the sheet and the compressed content. For a single sheet input, we may omit this wrapper and directly output its content object (or still use the array for consistency if multiple inputs are processed together).
    - **Content as Dictionary**: The primary content can be represented as a dictionary mapping **value descriptors** to **locations**. Each key is either a cell’s actual content (text/label) or an aggregated placeholder (format string or type token), and the value is the location or list of locations in the sheet. Locations will be given in Excel A1 notation. We will use single addresses for single cells and range notation for contiguous ranges. For example: a text label might appear once: `"Total": "A10"`, whereas a repeated label or value might be `"N/A": ["C5","C8","C12"]`, and a large numeric region could be `"#,##0.00": "B2:B50"` indicating that from B2 through B50 all cells share that format (thus that whole block is represented by the key `#,##0.00`). Another approach is to have separate sections in JSON for different categories (e.g., an object `{"anchors": [...], "values": {...}, "aggregates": [...]}`), but for simplicity, a single combined mapping is often used ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=To%20address%20these%20inefficiencies%2C%20we,merged%2C%20with%20empty%20cells%20excluded)) ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=depart%20from%20traditional%20row,usage%20while%20preserving%20data%20integrity)). We will choose the schema that is easiest to parse and aligns with how LLMs were fed the data (the paper’s description implies a dictionary JSON) ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=depart%20from%20traditional%20row,usage%20while%20preserving%20data%20integrity)).
    - **Markdown-like Elements**: Within JSON strings, incorporate markdown syntax to retain formatting:
        - **Bold/Italic**: If a cell was bold in Excel and it’s an important header (likely kept as an anchor), its text value in JSON can be wrapped with `**...**` to denote bold. Similarly, italic could be `*...*`. This way, if the JSON is later rendered or read by an LLM, it knows "Total" was bold (perhaps signifying it’s a total row header).
        - **Formulas**: If formulas are captured, we can include them in backtick code blocks. For example, a cell with formula `=SUM(B2:B9)` might be represented as `"=SUM(B2:B9)": "B10"` (mapping the formula string to its cell) or we include the formula alongside value. Alternatively, we may output formula results as values but still preserve the formula text as a markdown comment or in a separate field. Since the focus is on structure, we might by default treat formula outputs as regular values; however, including the formula could be a future option for completeness.
        - **Hyperlinks**: If a cell contains a hyperlink, output it as `[display text](URL)` in the JSON string. For instance, a cell with text *"Company Site"* linking to *example.com* becomes `"Company Site [http://example.com]": "A3"` or similar. This preserves the fact that the cell had an associated link.
        - **Lists or Multi-line Text**: If a cell contains multiple lines or some kind of list, we can translate that into markdown list syntax inside the string (using `- ` or `1. ` as appropriate). E.g., a cell with a bullet list could become "- item1<br>- item2" or just a single string with newlines that could be interpreted appropriately. Markdown headings (with `#`) could be used if a cell’s style or position implies a title, but since we already identify anchors and their importance, using bold might suffice for headers.
    - **JSON Example**: As an illustration, a portion of output might look like:
      ```json
      {
        "sheets": [
          {
            "name": "Sheet1",
            "content": {
              "Region": "A1",  
              "Product": "B1",  
              "**Total**": "F1",  
              "North": "A2:A5",  
              "South": "A6:A9",  
              "Widgets": "B2:B9",  
              "12345": "C2",  
              "12350": "C3",  
              "#,##0": "C2:C9",  
              "10/01/2023": "D2",  
              "10/02/2023": "D3",  
              "yyyy-mm-dd": "D2:D9"  
            }
          }
        ]
      }
      ```  
      In this hypothetical snippet, **Sheet1** has headers "Region", "Product", and "**Total**" (the last rendered bold in markdown). "North" and "South" are textual values appearing in A2:A5 and A6:A9 respectively (compressed as ranges). "Widgets" repeats in B2:B9. Column C had numeric data that we aggregated: rather than listing every distinct number, we show two examples and then a key "#,##0" covering C2:C9 to indicate all those cells are integers with that format (thus the intermediate values are abstracted). Column D had dates; we show two sample dates then use the `"yyyy-mm-dd"` format string as a key for D2:D9, representing all dates in that range. In practice, we might omit listing each distinct number or date entirely and *only* use the format key for the region, depending on how much detail we want to preserve – this can be tuned based on the balance of structural accuracy vs. token count. The JSON structure may also include meta-information (like original row/column counts, compression ratios) if needed for debugging, but the core idea is to provide a compact representation of the spreadsheet’s content and layout.

7. **Output Delivery**: Finally, the JSON is output to the specified location. By default, the tool can print the JSON to standard output, allowing the user to pipe it or redirect to a file. An `--output` option can direct it to save as a `.json` file. The JSON will be pretty-formatted with indentations for readability (unless a flag indicates to minify it). If multiple input files are processed in one run, the tool could output each to a separate file or all in one JSON structure keyed by filename. The CLI will communicate success or errors via exit codes and messages.

Throughout this pipeline, care is taken to maintain a **deterministic** transformation – given the same spreadsheet input, the output JSON will always be the same (the process doesn’t involve any randomness or AI-based summarization, only rule-based encoding). This ensures the compression is reproducible and debuggable, which is important for trust in a tool that might be used in data-sensitive contexts.

## Data Structures & Encoding
We will define clear data structures and JSON schemas to organize the compressed spreadsheet information. Below is the proposed JSON format and the use of markdown within it:

- **Top-Level JSON**: The output is a JSON object. If processing one file with one sheet, the JSON could directly represent that sheet’s content. For consistency and extensibility, we plan to always wrap sheets in a `"sheets"` array (or an object mapping sheet names to content). This way, multi-sheet workbooks and multi-file batches can be represented in one JSON. For example:
  ```json
  {
    "file": "workbook.xlsx",
    "sheets": [ ... ]
  }
  ```  
  could be used if we want to include the source file name. However, including file name is optional since the context might already be known. The key part is the `"sheets"` array containing sheet objects.

- **Sheet Object**: Each sheet entry will have at least:
    - `"name"` – the sheet name (e.g. "Sheet1", "January 2024", etc.).
    - `"content"` (or `"cells"` or `"data"`) – an object that holds the compressed cell information. This content object is essentially the inverted index mapping we constructed, possibly enriched with aggregated region markers. The keys in this object are **content descriptors** and the values are addresses or ranges:
        - **Keys**: They can be actual cell text (for textual cells that remain distinct) or a representative token for aggregated cells. We will ensure keys are strings. For numbers and dates, if we choose to use the format string or a placeholder, that will appear as the key. (We must be mindful that an actual cell text could coincidentally look like a format string or placeholder. To avoid confusion, we might decorate aggregated keys. For instance, we could use a prefix like `"<Date>"` or `"<Number>"` as the key instead of raw format, or some non-alphanumeric marker. Another approach is to keep a separate section for aggregated types rather than mixing in one object. The exact scheme will be decided to avoid key collisions. One simple rule: if a key matches one of our known type names or a typical format pattern, and that same string also appears literally in the sheet, we could differentiate them by context or by quoting. But such collisions are unlikely in practice – e.g., having an actual cell text "yyyy-mm-dd" is unusual.)
        - **Values**: These will typically be strings representing a cell address (e.g. `"F10"`) or a range (`"B2:B50"`). We use the Excel A1 notation because it’s familiar and concise. In cases where a key maps to multiple disjoint ranges, we have options: represent the value as an array of range strings, or as a single combined string with comma separation. JSON arrays are more structured, so we prefer something like `"N/A": ["C5","C8","C12"]` for non-contiguous addresses, and single string `"A2:A9"` for a contiguous range. We will also maintain the original ordering context where it matters – for example, anchors that appeared in a certain order (top to bottom, left to right) might be listed in that sequence when converting to JSON to preserve a sense of layout ordering. Keys in a JSON object are inherently unordered, but we might output them in a stable sorted order (maybe sorted by top-left occurrence in the sheet) to make the JSON deterministic and readable.

- **Including Markdown in Values**: Within the keys (which are content values from the spreadsheet), we will embed markdown notation as needed:
    - Text styles: If a cell was bold, italic, underlined, etc., the key string will include the corresponding markdown syntax (`**bold**`, `*italic*`, etc.). This way, a header like **Total** in Excel becomes `"**Total**": "F1"` in JSON, indicating not only the text but that it was emphasized. This helps preserve the hierarchy or emphasis that the original spreadsheet had.
    - Formulas: If we decide to include formula expressions as content keys (instead of or in addition to their calculated values), we will enclose them in backticks. E.g., a cell that showed `45` but is the result of `=SUM(B2:B10)` could be represented as ```"=SUM(B2:B10)`": "B11"``` (or possibly map the formula to the address and the value 45 also to that address, but that’s duplicate info). We might opt to just keep the value `45` in content and not the formula, to keep the JSON focused on visible data. However, including formulas as separate entries (perhaps under a `"formulas"` field) could be a future feature.
    - Hyperlinks: We’ll output them in `[text](url)` format. If a cell’s value is itself the URL or if we want to mark it, we could also just include the URL in parentheses. For example, a cell with text "Company" and a hyperlink could be `"Company (http://example.com)": "A4"` or `"[Company](http://example.com)": "A4"`. The latter is more explicit markdown but might complicate parsing slightly. We will likely use the pure markdown link syntax if the display text and URL are distinct.
    - Line breaks or bullets: If a cell contains a newline, in JSON it would be either an actual newline or `\n`. We can preserve it, but when displaying or using with an LLM, it might be better to replace newlines with explicit `\n` or some delimiter. Bullet points can be turned into markdown bullets (`- item`). However, multi-line cell content is probably rare in structured data, so this is a minor concern.

- **JSON Schema Summary**: In summary, the JSON might look like this (schema-wise):
  ```js
  {
    "sheets": [
      {
        "name": "<sheet name>",
        "content": {
           "<value or format>": "<address or range>",
           "<another value>": ["<addr1>", "<addr2>", ...],
           ...
        }
      },
      ...
    ]
  }
  ```  
  This schema balances **structural accuracy** (by preserving cell addresses and thus the layout positions) with **token minimization** (by consolidating duplicates and using placeholders). The use of markdown within the keys provides additional formatting context to the consumer of this JSON. The JSON structure is machine-friendly (keys and values easily parseable) while the markdown cues make it human/LLM-friendly by indicating emphasis and links.

- **Internal Data Structures**: Within the Scala implementation, we will use efficient data structures to build this JSON. For instance, the inverted index can be a `Map[String, List[CellAddress]]` internally. CellAddress might be a simple case class with (row, col, sheet) or just store the A1 string. We might accumulate ranges as a custom class too (startCell and endCell). For JSON output, we’ll convert these to the desired string format. Using a JSON library, we can create JSON AST objects or case classes that automatically derive JSON. Each sheet’s content map (with possible mixed value types for single vs multiple addresses) could be modeled as `Map[String, JsonValue]` where `JsonValue` is either a string or array of strings. We will ensure this structure is type-safe and easy to assemble.

In conclusion, the data encoding will result in a **compact JSON** representation of the spreadsheet, where the two-dimensional structure is implicitly encoded by cell addresses/ranges, and rich text features are embedded via markdown syntax. This structured output can be directly fed to an LLM or further processed, confident that it contains the essential layout and content of the original spreadsheet but in a much smaller token footprint.

## Algorithm Implementation Details
Here we break down how each of the core compression methods will be implemented in the Scala tool, including any necessary algorithms or data transformations. The implementation will closely follow the techniques introduced in SpreadsheetLLM’s SheetCompressor, ensuring we capture their benefits in a deterministic way:

- **Structural-Anchor Extraction**: This will be implemented using heuristic analysis of the sheet’s layout. The algorithm will:
    1. **Detect candidate boundary lines**: Iterate through all rows and columns to find those that stand out. We’ll measure attributes per row/col such as: number of non-empty cells, diversity of data types (e.g. does the row contain both text and numbers?), presence of merged cells or special formatting (borders, background color), etc. Rows/cols that have a significant contrast with their immediate neighbors on these metrics are marked as potential anchors ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=Initially%2C%20this%20method%20enumerates%20bounding,layout%20understanding%20of%20a%20spread)). For example, if Row 1 has many text cells (headers) and Row 2 is mostly numeric, that transition suggests Row 1 is a header row (anchor). Likewise, an empty row followed by a filled row might indicate a section break or title.
    2. **Enumerate table boundaries**: Using the candidate anchor rows and cols, the algorithm can form possible table region boundaries (pairs of anchors forming top/bottom and left/right of a rectangle) ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=sheet,characters%20in%20each%20row%20and)). We will use simpler logic in code: for instance, take the first and last identified anchor row as the sheet’s main top and bottom, and first and last anchor column as left and right boundaries. (If multiple tables per sheet are possible, a more complex enumeration of multiple regions would be needed, but an initial implementation can assume one main table region or treat multiple regions similarly by processing anchors in sequence.)
    3. **Apply filtering rules**: We will incorporate rules to validate that a candidate boundary really bounds a table. For example, check inside the boundary if there is a reasonable mix of data (not entirely empty), check if the supposed header row has a high text-to-number ratio, etc. ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=step%2C%20heuristics%20are%20applied%20to,characters%20in%20each%20row%20and)). In our implementation, we might simplify: if an identified anchor region is too small (say only one cell) or too large (spans entire sheet without any content difference), we adjust accordingly. The idea is to avoid false anchors.
    4. **Select anchors and neighbors**: After refining boundaries, decide the final set of structural anchor rows/cols. We include not just the exact boundary lines but also up to *k* neighboring lines around them ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=rows%20and%20columns%20that%20are,an%20ablation%20study%2C%20as%20detailed)) ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=65.0%25%20EoB,structural%20anchors%20to%20preserve%20ti)). This is important to capture closely related context (e.g. a header might actually span 2 rows, or there might be a sub-header right below the main header, etc. Including neighbors ensures these aren’t accidentally dropped). The parameter *k* is configurable; a default of 1 or 2 provides a good balance as noted in the paper’s ablation study ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=rows%20and%20columns%20that%20are,an%20ablation%20study%2C%20as%20detailed)). In code, this means for each identified anchor index, we also mark `index±1, ±2, ... ±k` as keepers (within bounds).
    5. **Prune the rest**: All rows and columns not marked as anchors (or within the neighbor threshold) are dropped from the data model. We will implement this by filtering out those cells from the in-memory list. We can either remove them in place or create a new filtered list of cells. If a whole row is dropped, all its cells go; if a column is dropped, all its cells go. After pruning, perform the coordinate re-mapping: essentially, we’ll rebuild the addressing by compressing gaps. E.g., if rows 5-100 were removed, what was row 101 becomes the new row 5 in the compressed model. We can maintain a mapping of old index to new index for both rows and columns. This step is straightforward: iterate through remaining rows in original order and assign new sequential indices. Do the same for remaining columns. Update each cell’s address to the new ones. This ensures the output addresses in JSON are contiguous (and maybe smaller like "A5" instead of "A101", which is cleaner).

  *Complexity*: The anchor extraction uses a few passes over the data (to compute metrics, then possibly to filter). This is O(N) for N cells, or O(R+C) if computed per row/col separately. We will implement it efficiently using vectorized operations where possible (e.g., computing each row’s type distribution in parallel). The heuristic can be refined over time, but even a simple approach (keep first few and last few rows/cols) can serve as a starting point, which already matches the intuition of preserving headers and summary rows. We will allow adjusting the aggressiveness via *k* or even turning off this step via CLI for full retention.

- **Inverted-Index Translation**: After anchor extraction, we have a condensed set of cells. The inverted-index translation is implemented in two stages as described: dictionary creation and address merging. In Scala, we can do this with a mutable Map or using groupBy on a collection of cells. Key implementation details:
    1. **Building the Map**: We iterate through each remaining cell (for each sheet). For each cell, take its content as a string key. If the cell contains a formula and we are not including formulas explicitly, we use the displayed value (or formula result) as the content. If the content is an empty string or null (shouldn’t happen after anchor step since we likely dropped empties, but just in case), skip it. Append the cell’s address to a list in the map for that content. We can use `Map[String, List[String]]` where the address is in "A1" notation. This can be done functionally (`cells.groupBy(_.content).mapValues(_.map(_.address))` in Scala) or with a loop. We prefer an immutable approach for clarity, but for very large data a mutable structure might be more memory efficient. The result is a raw inverted index: e.g. `"Acme Corp" -> ["A2","A15"]`, `"" -> ["B3"]` (empty string key we might remove in next step).
    2. **Remove Empties**: If any empty or blank text entries got through, drop them from the map. They are not needed since structural info of empties is implicit in the fact that certain addresses are simply not mentioned. This aligns with the method: *"empty cells excluded"* ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=format%2C%20where%20cell%20values%20serve,merged%2C%20with%20empty%20cells%20excluded)).
    3. **Merge Ranges**: For each map entry, we have a list of addresses. We will sort these addresses (first by sheet, then row, then column, though within one sheet we can assume sorting by row then col). Then we identify consecutive addresses. To merge, we need to detect runs in either a row or column. For instance: addresses `C2, C3, C4` form a vertical consecutive sequence (same column C, rows 2-4). We merge that as `"C2:C4"`. Similarly `D5, E5, F5` in the same row 5 would merge to `"D5:F5"`. We have to be careful that something like `C2, C3, C5` is not fully consecutive (there’s a break at C5 skipping C4), so that would become `"C2:C3", "C5"`. We can implement this by converting addresses to numeric indices (e.g., C2 -> (row=2, col=3) assuming A=1, B=2, etc.), then scanning. If the next address is exactly one row below or one column to the right (with same col or same row respectively) as the current, it’s part of a run. We continue until the run breaks, then output a range. This may result in multiple ranges per key if the value appears in separate blocks. We then replace the list in the map with either a single string (if one address or one range) or an array of strings (if multiple ranges) when writing JSON. (If using a JSON library, we might keep it as a JSON array of strings for multiple, or always as array for uniformity, but single string is simpler for single address cases.)
    4. **Output Map**: The final inverted index is essentially our JSON "content" object for the sheet, before aggregation. At this point, every key is a concrete value from the sheet. This stage is lossless; it has **preserved all remaining cells exactly**, just reorganized them ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=depart%20from%20traditional%20row,usage%20while%20preserving%20data%20integrity)). If we stopped here, the JSON would contain all anchor cell values verbatim and all their addresses, already saving a lot of space by not listing empties and by consolidating duplicates. However, we proceed to the next step to compress further the large numeric sections.

  *Implementation note*: We will ensure that special characters in keys (commas, quotes, etc.) are properly escaped or handled by the JSON library. Also, keys that are very long (e.g., a paragraph in a cell) will just appear as-is (possibly making the JSON bulky for that one entry, but that’s unavoidable unless we choose to truncate or summarize text, which is beyond deterministic compression). The merging algorithm complexity is O(M log M) per key in worst case (M addresses for that value) due to sorting; in practice, values that appear many times tend to be things like empty strings or common entries which we might handle efficiently. The overall number of cells is greatly reduced by the anchor step, so this is manageable.

- **Data-Format-Aware Aggregation**: The implementation of this part will follow a rule-based approach for identifying types and a flood-fill for clustering, as discussed earlier. Key steps in code:
    1. **Identify Cell Types**: We’ll create a function `inferType(cellValue: String, formatCode: Option[String]): DataType` that returns an enum or string label for the cell’s type. It will check, in order: if a formatCode (NFS) is provided by POI and is specific (not "General"):
        - If the formatCode corresponds to a date/time (contains date/time placeholders like `d, m, y, h, s`), label as "Date" or "Time" accordingly.
        - If it’s a currency format (contains `$` or other currency symbol or set to currency category), label as "Currency".
        - If percentage format, label "Percentage".
        - Scientific format (often indicated by an "E" or by POI as such), label "Scientific".
        - If format is an integer or decimal pattern (like `0` or `#,##0.00`), we might use "Integer" vs "Float" based on presence of decimals.
        - If format is a known custom type like phone number or zip code, those might just fall under "Others" since not listed among the main ones.
          If no useful NFS is present or it’s general: we then examine the cell’s value string itself:
        - If it parses as an integer (and maybe within a certain range, e.g., 4-digit could be Year), check length == 4 and in a reasonable range to classify "Year".
        - If it’s a numeric string with decimals, classify "Float". If no decimal, "Integer".
        - If it contains `@` and maybe a dot, classify "Email".
        - If it matches common date patterns (e.g. "MM/DD/YYYY" or "YYYY-MM-DD" etc.), classify "Date".
        - If it matches time pattern (like "12:30" or "12:30 PM"), "Time".
        - If it’s something like "5.23E-4" or similar, "Scientific".
        - If it’s purely alphabetic or mixed, then not a numeric format – those likely remain as textual "Others" (or if it’s a known categorical format like state codes, we still call it "Others").
          We also include a category "Others" for anything that doesn’t fit the above (including blanks, but blanks we dropped already). This mapping will replicate the paper’s predefined types ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=ever%2C%20spreadsheet%20users%20do%20not,world%20corpora)).  
          After this, each key from the inverted index (which represents a cell’s content) can be associated with a DataType label. We will do this only for those keys that represent *individual cells or small clusters that are candidates for aggregation*. In practice, these are the **numeric or date values** that appear in bulk. Textual keys (like category names) we likely want to leave alone. We can decide to only aggregate types like Number, Date, etc., and ignore textual types in this step.
    2. **Clustering**: We need to cluster adjacent cells of the same type. However, at this point in the pipeline, we don’t have a 2D grid readily available, because we’ve transformed data into a dictionary. We will therefore go back to a grid view of the sheet’s remaining cells (the skeleton). We can reconstruct a boolean grid or a type grid of size (rows_remaining x cols_remaining). Alternatively, we still have our in-memory data model (pruned to anchors) from step 3, which retains the positional info of each cell. We can use that: iterate through the pruned sheet’s coordinate system and build a 2D array of type labels (for simplicity, maybe a Map[(row, col) -> type]). Then perform a DFS/BFS on this map to find connected components of identical type. This is basically the Algorithm 1 in the appendix, which we can implement similarly ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=5%20Function%20dfs,c%5D%29%20%E2%88%A8%20val_type%21)) ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=15%20for%20r%20%3D%200,c%5D%20then%2018%20val_type%E2%86%90)). We will ensure to only cluster if the region is larger than 1 cell – if it’s a single cell, we might not aggregate it (since replacing a single value with its type provides no token savings and loses information). So one rule: only aggregate clusters above a certain size (e.g., >2 cells, or at least one full row/col of data).
    3. **Replace Values with Type Descriptors**: For each cluster found, we determine a descriptor to use as the key in JSON:
        - If all cells in the cluster share an NFS or a specific format, we can use that format code (e.g. `"$#,##0.00"` for a currency region) as the key. This is nice because it encapsulates both type and style.
        - If the cluster spans multiple formats but our rule-based classifier labeled them the same type (e.g., some numbers had 2 decimals, some had 3, but all were classified Float), then we use the broader type label like "Float". We might append a generic format like `<Float>` or just use "Float".
        - For dates, if a uniform format (all YYYY-MM-DD), use that; if mixed (some MM/DD/YYYY, some DD-MMM-YY), we could just use "Date".
        - We will standardize these descriptors, possibly enclosing them in angle brackets or some notation to distinguish them from real cell text. Perhaps using `<Type>` (e.g. `<Date>`, `<Integer>`) could be a safe approach to avoid collision with actual data. However, the paper’s description implies using the actual format strings as the representation ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=3,format%20strings%20and%20data%20types)), which an LLM could understand (they even visualized rectangular regions labeled by format strings). We could do the same: use format if available, else type name.
        - We then need to remove the individual entries for all those cells from the content map and replace them with one entry for the whole region. For example, suppose we had keys "18476" and "18674" for cells C2 and C3 in the dictionary. If we cluster them as part of a region C2:C10 of type "Number" with format "#,##0", we would remove "18476" and "18674" (and all other values in C2:C10) from the content map, and instead insert a new key `"#,##0"` (or "<Number>") with value `"C2:C10"`. This replacement is how we achieve the token reduction. It’s important that this aggregation is done after building the dictionary, because we needed the distinct values to identify type and because removing them earlier might lose anchor information if any of those values were anchors (though typically anchors are text labels, not large numeric lists). In case an anchor cell (like a total) is numeric and would be clustered, we should probably *exclude* it from aggregation to keep it distinct (since it might be an outlier anyway, not contiguous with the data block or styled differently).
        - We will implement the removal/replacement carefully: iterate through each identified cluster region, and for each cell in that region, if its value key exists in the map, remove it (or mark for removal). Then add the new entry for the region’s descriptor. There is a choice: if a cluster’s cells had multiple distinct values, they currently exist as multiple keys in the map. Removing them and adding one reduces keys count. If some of those values also appeared outside the region, we must be cautious not to remove those occurrences. However, by definition, our clusters are contiguous areas – if a value appears both in the cluster and somewhere else non-adjacent, we *should not* aggregate the distant occurrence. That distant occurrence would be a separate dictionary entry and not part of this cluster’s removal. So effectively, we are only removing keys that correspond to cells inside the region and not touching identical values elsewhere. This means our data structure might need to know not just the value but the specific addresses to remove. It could be easier to rebuild the content map from scratch after aggregation: e.g., take the list of clusters, mark those cells as handled, and then rebuild a new map of content for everything not in an aggregated cluster. We will choose an approach that’s simpler to implement correctly (perhaps rebuilding to avoid tricky state updates on a map while iterating).
        - After inserting aggregated entries, we’ll have a final content map ready for output. Each aggregated entry effectively compresses many values into one.

    4. **Preserve structural accuracy**: We will ensure that we do not aggregate away any value that is critical for understanding. Typically, text labels and unique identifiers remain separate. We will not cluster the "Others" type aggressively because that might include miscellaneous text. The focus is columns of similar data (usually numbers or dates). By keeping anchors and distinct texts separate and only aggregating the homogeneous data regions, we follow the paper’s guideline of balancing token count vs. structure ([Microsoft's SpreadsheetLLM: Revolutionising Spreadsheet Analysis with AI](https://www.ryanmcdonough.co.uk/microsofts-spreadsheetllm-revolutionising-spreadsheet-analysis-with-ai/#:~:text=1.%20Structural,component%20recognises%20patterns%20in%20data)) ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=values%20are%20not%20essential%20for,rules%20to%20match%20the%20value)). The structural anchors ensure the table layout (headers, boundaries) is intact, and the aggregated regions ensure that the nature of the data in each section is conveyed (e.g., “this area is a bunch of dates”) without listing every item.

  In code, this module will likely use nested loops or recursion for the DFS clustering. We’ll optimize by marking visited cells in a set or a boolean matrix. The type inference is O(N) over cells, and clustering is also O(N). Removing and rebuilding the map is also O(N). Thus, the overhead is linear in the number of remaining cells, which after anchor extraction is much smaller than the original spreadsheet size (often a 25× reduction in cell count) ([SpreadsheetLLM: Encoding Spreadsheets for Large Language Models - Microsoft Research](https://www.microsoft.com/en-us/research/publication/encoding-spreadsheets-for-large-language-models/#:~:text=spreadsheet%20table%20detection%20task%2C%20outperforming,LLMs%E2%80%99s%20performance%20on%20spreadsheet%20data)).

- **Parallelization in Algorithms**: We will incorporate parallel processing in some of these steps (detailed in the next section). For instance, type detection for each cell can be parallelized easily, as can the initial map building for inverted index (using parallel collection to group by). The anchor detection involves comparing neighbors, which is somewhat sequential, but computing stats for each row/col can be parallel. We will make sure that any parallel processing doesn’t break the deterministic nature or ordering where it matters. The algorithms as described will yield the same result regardless of execution order, as they rely on set operations and groupings that are commutative (the only difference might be iteration order in the output JSON, which we’ll sort for consistency).

- **Balancing Token Minimization vs Structural Accuracy**: The implementation of these algorithms will include tunable parameters (like the anchor threshold *k*, or whether to enable full aggregation) so users can adjust the output detail. By default, we aim to maximize compression while **preserving all structural cues**: anchors ensure important cells remain verbatim, and type aggregation ensures that we don’t waste tokens on every number while still indicating their semantic category ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=values%20are%20not%20essential%20for,rules%20to%20match%20the%20value)). This addresses the user’s requirement of balancing the two – our default will mimic the paper’s approach (which proved high accuracy with massive compression), and we will document how to dial it back if needed (for example, an option to output actual numbers instead of type placeholders if exact values are required for some use-case).

Overall, these implementation details ensure that each method – structural anchor extraction, inverted-index translation, and format-aware aggregation – is faithfully realized in code. Each contributes to shrinking the output size: anchors cut out bulk data, the inverted index removes redundancy, and format aggregation generalizes repetitive patterns. Combined, they achieve a strong compression (the paper’s SheetCompressor achieved ~96% token reduction ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=We%20conducted%20a%20comprehensive%20evaluation,surpassing%20the%20previous%20SOTA%20method)), which we can strive for) while still retaining the essence of the spreadsheet’s information structure.

## Parallel Processing Strategy
To efficiently handle large spreadsheets, the tool will leverage parallel processing at multiple levels. Scala’s concurrency utilities (parallel collections, Futures, or Akka streams/actors) will be used to utilize multiple CPU cores. The parallelization strategy includes:

- **Sheet-Level Parallelism**: If the input file contains multiple sheets (or if multiple files are processed in one run), we can process each sheet in parallel. Each worksheet’s compression is independent of others (there may be slight coupling if we output a single JSON – we’ll need to merge results – but the heavy work per sheet can be done concurrently). We will use a parallel collection or Futures for the list of sheets. For example, `workbook.sheets.par.map(compressSheet)`. This can speed up multi-sheet workbooks significantly on multi-core machines.

- **Partitioning Large Sheets**: For very large sheets, we can divide the workload by rows or columns for certain computations. Strategies include:
    - *Row-chunk Processing*: Split the set of rows into chunks (e.g., by thousand rows) and process each chunk on a different thread. This can be applied when calculating row statistics for anchor detection or when building the inverted index. For example, each chunk can produce a sub-map of value->addresses, and then we reduce/merge the sub-maps at the end. The merging step (combining maps) can also be parallelized in a tree-reduction manner if needed.
    - *Column-based tasks*: Similar chunking can be done by columns if needed (less common since typically more rows than columns). But for column-oriented stats (like detecting anchor columns), splitting column analysis across threads could help.
    - *Cell-wise parallel tasks*: Operations like inferring data type of each cell or scanning for differences can be done in parallel on each cell or on each row. Scala’s parallel collections can turn a `.map` over cells into a parallel map easily. We must be cautious with very large collections (millions of elements) as spawning too fine-grained parallel tasks can add overhead. We may chunk them manually for better control.

- **Parallel Anchor Detection**: The anchor-finding heuristic involves examining each row and each column. We can compute metrics (like "how many text vs numbers in this row", "does this row have merged cells", etc.) for all rows in parallel – e.g., use `.par` on the list of rows. Once we have a vector of metrics, determining anchors by comparing neighbors is mostly sequential (you need row i vs row i+1), but that’s a negligible cost compared to computing the metrics themselves. Similarly, analyzing columns can be parallelized. This will speed up the identification of structural anchors in wide or long sheets.

- **Parallel Inverted-Index Building**: Building the dictionary is effectively a grouping operation. We can split the cells into batches and have each batch do its own grouping. In Scala, `groupBy` is not parallel by itself, but we can do a `.par` on the sequence of cells and then group. Another approach: use a thread-safe concurrent map and have threads insert into it. This needs proper locking or using Java’s `ConcurrentHashMap` with `computeIfAbsent` to accumulate addresses. The functional way (parallel groupBy on immutable structures) might be simpler to implement. After parallel grouping, merging partial results (if any) might be needed – e.g., if two batches had the same value key, we need to concatenate their address lists. That can be done safely by a reduce operation. Given that grouping by value is a key part, we might also consider using Scala parallel collections which handle such combining behind the scenes.

- **Parallel Aggregation**: Determining cell types for each cell is embarrassingly parallel – each cell’s type can be inferred independently. We will use `.par.map(inferType)` on the cell list to assign types quickly. Clustering those cells by adjacency is a bit trickier to parallelize because DFS is inherently depth-first and could thrash if done from multiple starting points concurrently. However, we can optimize by starting DFS from multiple seeds in parallel as long as they don’t collide. The algorithm from the paper (Algorithm 1) processes cell by cell and uses recursion/DFS ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=15%20for%20r%20%3D%200,c%5D%20then%2018%20val_type%E2%86%90)). We could attempt to launch separate DFS threads from different starting points, but we’d need to ensure they don’t step on each other marking visited cells. The overhead might not be worth the complexity for clustering, because the clustering is typically not the heaviest part (especially since anchor extraction has cut down the size). For simplicity, we might run the clustering single-threaded and focus parallel efforts on the type inference and earlier stages. If profiling shows clustering as a bottleneck, we could revisit splitting the sheet into regions (e.g., do DFS in different quadrants of the sheet concurrently).

- **I/O Parallelism**: Reading the spreadsheet file from disk might be I/O bound. Apache POI itself isn’t inherently asynchronous, but we can overlap reading with processing if we use a streaming approach. For example, using the SAX event API for .xlsx, we could push rows into a processing queue as they are read. In a pipeline fashion, one thread (or IO-bound task) reads rows, while worker threads consume those rows to build the model or even directly build partial index. This is an advanced optimization and might complicate things (ensuring ordering for anchors). A simpler approach is to read the entire content sequentially (likely fast enough from an SSD for tens of MBs files) and then parallelize computation on the content, which is what we described above.

- **Thread Management**: We will allow the user to specify a degree of parallelism if desired (e.g., `--threads 4`). By default, we can use `Runtime.getRuntime.availableProcessors()` to choose an appropriate number of threads. Scala’s ForkJoinPool (used by parallel collections) will handle scheduling. We should also be mindful of not using too many threads on hyper-threaded CPUs for large tasks due to memory thrash. We might default to something like min(availableProcessors, 8) as a sane default.

- **Memory Considerations**: Parallel processing means multiple data copies might exist transiently (each thread handling a portion). We will try to use shared structures read-only across threads when possible (the original cell list can be read by all threads if it’s in memory). Using `.par` on an immutable collection is beneficial here. We should also consider that Apache POI objects (like cell objects) are not thread-safe; however, once we extract values and formats into our own lightweight case class, those are safe to use concurrently.

- **Concurrency and Determinism**: We’ll ensure that parallelism does not introduce nondeterminism in the output. For instance, if threads process parts of the dictionary, we must combine results in a stable order. We will likely sort the final keys or addresses before output to avoid random ordering due to hashmaps. The content of each key is the same regardless of thread timing, since we’re not doing anything like race conditions (we will avoid any situation where two threads could try to modify the same entry concurrently without synchronization). Using functional transformations (map/reduce) helps guarantee the outcome is deterministic.

In summary, the tool will utilize parallel processing primarily to speed up **reading large files**, **computing sheet metrics**, **building the inverted index**, and **inferring data types**. This will make the tool scalable to spreadsheets with tens or even hundreds of thousands of rows. For extremely large spreadsheets (millions of cells), the combination of anchor-based pruning and parallel execution ensures we handle them as efficiently as possible on a modern multi-core system.

## CLI Interface Design
The command-line interface will be designed for ease of use and flexibility, following typical UNIX CLI tool conventions. Below we outline the expected commands, options, and user interaction model:

- **Command Invocation**: The tool will be invoked via a main class or script, for example:
  ```bash
  $ spreadsheet-compressor [options] <input-file1> [<input-file2> ...]
  ```  
  We can provide a short alias like `sheetcompress` for convenience. The CLI will accept one or multiple input files. If multiple files are given, the tool will process each (possibly in parallel) and either output combined results or separate outputs (see “Output modes” below).

- **Basic Required Argument**:
    - **Input File(s)**: Path to the Excel file(s) to process. These can be absolute or relative paths. We will support common formats by extension: `.xlsx`, `.xls`, `.xlsm`, `.xlsb`. If a directory is provided instead, we could optionally process all spreadsheet files in that directory (perhaps with a flag like `--recursive` or `--all` to opt-in). For initial implementation, we focus on files.

- **Options and Flags**:
    - `-o, --output <path>`: Specify output file path. If not provided, default is to print JSON to stdout. If multiple input files are given, this could be either a directory or a single file. We might not allow a single output file for multiple inputs unless we wrap all results in an array; an alternative is to require `--output-dir` in that case to place each JSON in that directory with an appropriate name. We will document that behavior.
    - `-k, --anchor-threshold <n>`: Set the structural anchor neighborhood threshold *k*. This controls how many neighboring rows/columns around anchors are kept ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=rows%20and%20columns%20that%20are,an%20ablation%20study%2C%20as%20detailed)). Default might be 1 (keeping anchors and immediate neighbors). Users can increase it (e.g. 2 or 3) to preserve more context (at cost of more tokens) or set 0 to keep only the exact anchor lines. They could even disable anchor compression by setting a very large k or using a separate flag.
    - `--no-anchor` or `--full` (Boolean flag): Bypass structural anchor compression. If this is set, the tool will skip step 3 and keep the full sheet (except it may still drop totally empty rows/cols to avoid trivial noise). This might be useful for small sheets or if the user trusts the LLM’s token limit is sufficient. It essentially forces k = infinity.
    - `--no-index` (flag): Bypass inverted-index compression. This would preserve the grid structure in output (likely outputting a row-by-row JSON instead of the dictionary). We expect most users won’t use this, but it’s there for completeness or debugging. (If both anchor and index compression are disabled, the output would be a near-raw JSON of the sheet, which might be huge, but could be used to verify correctness against the original.)
    - `--no-aggregate` (flag): Disable data-format aggregation. This means do not cluster numeric cells; all values will appear as-is in the JSON (they may still be deduplicated by the inverted index though). Use this if exact values are needed in output. By default, aggregation is on to maximize compression.
    - `-t, --threads <N>`: Limit the number of threads to use. By default, we auto-detect cores. This allows users to reduce concurrency (for example, to avoid saturating a machine running other tasks) or sometimes increase if auto-detect is not optimal. If set to 1, it effectively forces single-threaded execution (useful for debugging or comparison).
    - `--sheet <name or index>`: If the user only wants to compress a specific sheet in the workbook, this option can specify it by name or index (0-based or 1-based index TBD). By default, all sheets are processed.
    - `--format <json|markdown>`: This could allow choosing output format. Our plan is to output JSON with markdown elements by default (since that’s required), but we might allow a pure Markdown output as an experimental feature. However, the problem specifically asks for JSON, so JSON is the default and likely the only supported format in the first version. We can omit this option in initial release.
    - `-q, --quiet`: Run in quiet mode, minimal logging (only errors). This is useful if piping output and you don’t want extra info.
    - `-v, --verbose`: Enable verbose logging/debug info. Could be repeated (e.g., -vv for even more). This will print details about what the tool is doing (like how many anchors found, how many cells removed, etc.). Useful for developers or power users to understand the compression outcome.
    - `--version`: Print the tool version and exit.
    - `-h, --help`: Show usage information.

- **Output Behavior**: By default, the JSON is printed to standard output. The user can redirect it to a file. If `--output` is provided with a filename and a single input, JSON goes to that file. If multiple inputs and `--output` is a directory, we will create one JSON file per input (naming them `inputfilename_compressed.json` or similar). If multiple inputs and a single output file is given, we might package results in an array or object with file names as keys. We will clarify this in documentation; to keep it simple, we might disallow single-file output for multiple inputs to avoid confusion.

- **User Interaction Model**: The tool is meant to run and terminate (no interactive prompts). It will validate arguments and either run the compression or print an error message if something is wrong. For example, if the file extension is not recognized as a spreadsheet, we’ll output an error "Unsupported file format". If the file doesn’t exist: "File not found". On success, the exit code is 0 and the JSON is produced. On failure, exit code non-zero and error message to stderr.

- **Examples**: We will provide usage examples, such as:
    - Compress a single file, print to console:
      ```bash
      $ sheetcompress data.xlsx > data.json
      ```  
    - Compress with more context (k=2) and disable aggregation (keep numbers):
      ```bash
      $ sheetcompress --anchor-threshold 2 --no-aggregate large.xlsx -o large_out.json
      ```  
    - Compress all Excel files in a directory to separate outputs:
      ```bash
      $ for f in reports/*.xlsx; do sheetcompress "$f" -o "compressed/${f##*/}.json"; done
      ```  
      (In future, we might add a built-in directory processing feature to avoid needing shell loops.)

- **CLI Implementation**: We’ll likely use a library (like **scopt** or **picocli**) to parse these arguments in Scala. This will handle help text generation and make it easy to retrieve option values. The main program will then call into our library functions (e.g., a `compressWorkbook(file, options)` function) and handle output.

- **Interactivity and Logging**: As a CLI, everything is non-interactive once executed. We will output logs to stderr so that JSON (on stdout) isn’t polluted by log messages. For example, if verbose mode is on, progress info like "Loaded 3 sheets, starting compression..." goes to stderr. In quiet mode, only critical errors (if any) go to stderr.

- **Error messages**: We will make error messages clear and actionable. For instance, if Apache POI throws an exception for an encrypted file or macro issue, we catch it and tell the user "Unable to read <file>: possibly encrypted or corrupted Excel file." If a .xlsb is given and somehow our code can’t handle it, we might say "XLSB support is limited – please convert .xlsb to .xlsx and retry" (though we intend to support .xlsb via XSSFBReader). We will also handle out-of-memory gracefully if possible: catch `OutOfMemoryError` and advise "The file is too large to process with current settings; try enabling anchor compression or increase memory."

The CLI design thus emphasizes a smooth user experience: straightforward commands, sensible defaults that implement all compression methods, and flexibility to tweak or disable parts of the compression. This ensures both casual users and advanced users (with specific needs) can use the tool effectively.

## Performance Considerations
Building a tool for large spreadsheets demands careful attention to performance to avoid long runtimes or excessive memory usage. Below, we address potential bottlenecks and our strategies to mitigate them:

- **I/O and Parsing Overhead**: Reading Excel files can be heavy, especially for older `.xls` or very large `.xlsx`. We use Apache POI which is robust but can be memory intensive. To improve performance:
    - We will use POI’s **streaming** modes when possible. For `.xlsx/.xlsm`, POI’s SAX (event) API or the SXSSF (Streaming Workbook) can read rows in a forward-only manner with low memory footprint. For `.xlsb`, the XSSFBReader already works in a streaming fashion ([XLSBUnsupportedException (POI API Documentation) - Apache POI](https://poi.apache.org/apidocs/dev/org/apache/poi/xssf/XLSBUnsupportedException.html#:~:text=We%20don%27t%20support%20,of%20xlsb%20files%20via%20XSSFBReader)). Using these, we can parse huge files without loading the entire workbook into memory. We might implement a hybrid: first pass to identify anchors (which might require scanning a lot of cells but not storing them all), then a second pass to actually gather data for the kept cells. This two-pass approach trades some extra file reading for much lower memory usage.
    - If using the usermodel (XSSFWorkbook) for simplicity initially, we must be mindful that it loads everything into RAM, which can be slow and memory-heavy for big files. We will document guidelines (e.g., for >100k rows, use 64-bit JVM with ample heap). In future, we can optimize by switching to event parsing.
    - Also, using the appropriate POI factory for each format avoids unnecessary conversion. We’ll ensure we don’t, say, read a .xls into XSSF (which would convert internally); instead we use HSSF for .xls.

- **Memory Footprint**: The biggest memory usage comes from storing cell data. With anchor extraction, we drop a lot, but we need to identify what to drop. If we load the whole sheet then drop 75%, we still had to hold it initially. For extremely large sheets, we might implement anchor detection on the fly (without storing all cells): e.g., stream through rows, compute row heterogeneity metrics and keep only anchor candidates in memory. After that, stream again to collect those anchors’ cells. This would greatly reduce memory usage by never holding the full sheet at once. This approach is complex but feasible. Our plan is to first implement a simpler approach (load then prune) and optimize if needed. We will at least free references to dropped cells immediately after anchor selection to allow GC to reclaim memory.
    - Using Scala collections: We should be careful with large lists and maps. For instance, a huge Map of all cell values could consume a lot of memory for keys and values. But since many cells are pruned, and many values repeat, it’s not as bad as the raw data size. We can also consider using more memory-efficient data structures (e.g., using arrays of primitives for large numeric sequences, but since we output JSON, we need them as strings eventually anyway).
    - We will avoid holding multiple full copies of data at once. For example, when building the inverted index map, we can directly populate the final JSON structure rather than first storing a giant intermediate structure and then copying. We can stream out to JSON as we determine entries, but since we want to do aggregation later, we do need a structure we can modify. We might keep references manageable by, say, removing entries from the map as we aggregate them (to not hold them twice).

- **CPU Hotspots**: The major computational tasks are scanning cells, grouping, and clustering.
    - **Scanning cells** for anchors and building the dictionary is linear in the number of cells. For a sheet with, say, 1 million cells, this is on the order of 10^6 operations, which is fine in Java/Scala if done efficiently (especially with primitive operations or well-optimized library calls). The use of parallel processing will cut wall-clock time. We will also use efficient string handling – e.g., avoid unnecessary string concatenations. (POI might give us cell content as already a String or a double for numeric; we’ll convert to string likely once when adding to map. We should use `StringBuilder` or similar if constructing addresses frequently, or reuse address strings as keys rather than recompute them each time, maybe caching the address of cells as they are processed.)
    - **Grouping (Map operations)**: A large number of unique keys or a large number of insertions into a map can be a bit heavy. We might see performance drops if a sheet has mostly unique values (like many unique IDs). A hash map insert is O(1) average, so even 1e6 inserts is fine, but memory allocation for growing the list of addresses might add overhead. We will pre-size the map if possible (if we know roughly how many cells remain after anchors, we can allocate that size to avoid rehashing). Also, using an array or list for addresses that grows can cause re-allocations; we might accumulate addresses in a mutable ListBuffer or similar then freeze to list at end to minimize constant copying.
    - **Sorting for ranges**: Sorting address lists for range merging is typically minor compared to grouping, unless a single value appears in, say, 100k cells. If that happens (e.g., a column of 100k zeros), sorting 100k addresses is not too bad (maybe a few milliseconds), and merging them is linear. So this is acceptable.
    - **DFS clustering**: If the sheet after anchors still has, say, 10k cells of type "Number" contiguous, the DFS will traverse all 10k, which is fine. If there are many small clusters, the overhead of recursion repeatedly is okay because overall it touches each cell a bounded number of times. We will ensure our recursion is tail-optimized or use an explicit stack if needed to avoid deep recursion stack issues (a very large cluster could cause deep recursion – but if we do DFS from top-left to bottom-right in a cluster, the depth is at most the number of cells if it snakes, which could blow the stack for e.g. 10k elements. We might implement clustering with an explicit Stack/Queue to be safe for large clusters).

- **Token Count vs Accuracy**: While not a performance issue in the traditional sense, one consideration is ensuring the output size is indeed minimized to be suitable for LLM input. We will check that our JSON with markdown is succinct. If we see opportunities to shorten it further without losing meaning (for example, maybe using short placeholders like "<Int>" instead of "Integer"), we might do that. JSON itself adds some overhead (quotes, braces). The user specifically requested JSON, so we stick with it, but a pure markdown or CSV could be slightly shorter. We assume JSON is fine given the large compression achieved.

- **Garbage Collection and Object Allocation**: The tool will create lots of small objects (strings for addresses and values). We should be mindful of Java GC overhead. Where possible, reuse objects: e.g., reuse `StringBuilder` for constructing addresses; reuse the same string for identical content (Java’s string interning or using the fact that POI might give same String reference for repeated cell text – not sure if it does). We can also hint the JVM with `-Xmx` for needed heap. If memory is tight and GC becomes an issue, we could consider streaming output directly for some parts to avoid holding everything (but aggregation complicates that).
    - We will test the tool on large files and optimize any glaring slow parts. For instance, if building the map is slow due to synchronization in parallel, we might switch to sequential for that part if it’s actually faster due to less overhead (sometimes parallel isn’t worth it for certain sizes).

- **External Factors**: We should note that extremely complex spreadsheets with thousands of styles or very deep cell dependency (formulas) might slow down POI when reading (if formulas need evaluation). By default, we are not evaluating formulas, just reading stored value or formula string, so that avoids a huge performance hit (evaluating formulas would require bringing in the calculation engine of POI which is slow for big sheets). So we will skip formula evaluation (just treat them as data as-is).

- **Profiling and Bottleneck Handling**: We will include logging at verbose level for the time taken in each stage (anchor detection, index building, aggregation) to help identify if any stage becomes a bottleneck on real data. If, for example, anchor detection is taking too long, we might simplify the heuristics (maybe not check too many format features). If reading .xlsb via XSSFB is slow, we might suggest converting it externally as a workaround. If memory is a problem, we’ll document the need to increase heap or use streaming mode.

In summary, the plan addresses performance by **reducing data early (anchor prune)**, **avoiding redundant work (inverted index and merging)**, **leveraging parallelism**, and using efficient algorithms and data structures. The end result should be a tool that runs in a reasonable time (e.g., a few seconds for moderately sized files, and scaling linearly for larger ones) and handles large spreadsheets within practical memory and time limits, all while producing a highly compressed representation.

## Error Handling & Logging
Robustness is crucial for a CLI tool that may be fed arbitrary real-world spreadsheets. We will implement comprehensive error handling and logging to ensure the tool fails gracefully and provides useful feedback. Key strategies include:

- **Input Validation**: Upon startup, the CLI will validate that the input file path(s) exist and have a supported extension. If an unsupported format is detected (e.g., a CSV or ODS file, or a random file with .xlsx extension but not actually Excel), we will output an error like: *"Unsupported file format or file is not a valid Excel spreadsheet."* We may also perform a quick file header check (OOXML zip header or BIFF signature) for more certainty and catch exceptions from POI if the file is corrupt.

- **Apache POI Exceptions**: Interacting with POI can throw various checked and unchecked exceptions, for example:
    - `EncryptedDocumentException` if the file is password-protected. We will catch this and inform the user that the file is encrypted and cannot be processed (unless we add an option to supply a password in future).
    - `OLE2NotOfficeXmlFileException` or similar, if the file extension says .xls but content is not an Excel or vice versa. Error: "File format and content mismatch or file is corrupted."
    - `XLSBUnsupportedException`: If someone tries .xlsb and we inadvertently use XSSFWorkbook, POI will throw this indicating .xlsb isn’t supported in that API ([XLSBUnsupportedException (POI API Documentation) - Apache POI](https://poi.apache.org/apidocs/dev/org/apache/poi/xssf/XLSBUnsupportedException.html#:~:text=XLSBUnsupportedException%20%28POI%20API%20Documentation%29%20,of%20xlsb%20files%20via%20XSSFBReader)). Our design avoids this by using XSSFB, but we’ll still put a catch to advise using proper support.
    - Out of memory (OOM): Not exactly POI throwing, but if we run out of heap mid-process, the JVM will throw an OutOfMemoryError. We can catch Errors in the main method (catch Throwable broadly) to handle this. If OOM happens, we will print: "Error: Ran out of memory while processing. Try increasing the JVM memory or enabling stronger compression (to reduce data).". Then exit non-zero. Since continuing after OOM is not really possible, this is mostly to let the user know what happened rather than just crashing silently.
    - If any unexpected exception occurs (NullPointer, etc.), we will catch it at a top-level and print a stack trace if in debug mode, or a generic "An unexpected error occurred. Please run with --verbose for more details." in normal mode. That ensures the tool doesn’t just die without explanation.

- **Data Edge Cases**:
    - **Empty Sheets**: If a sheet has no data at all (completely empty or just blank cells), our output for that sheet might be an empty content object. We should handle this gracefully (perhaps output `"content": {}` or skip the sheet). We’ll log a warning: "Sheet X is empty, skipping." if that occurs.
    - **Single-cell Sheets**: If only one cell with a value exists, anchor detection will mark it as anchor anyway, and everything proceeds. Just ensure ranges and aggregation handle the trivial case (they should; the inverted index will just have one entry).
    - **All cells identical**: e.g., a sheet where every cell says "N/A". Anchor detection might pick the first row/col as anchors, remove the rest if k=0. Even if k>0 keeps some, inverted index will then map "N/A" to a huge range (or multiple ranges). That’s fine. We just ensure the range merging code doesn’t choke on extremely large ranges.
    - **Formula cells**: If we treat formula content as value, and some formulas result in empty (like a formula that outputs blank), those might appear as empty string. We’ve decided to skip empties. If a formula error occurs or POI can’t get a formula’s cached value, POI might give a special value or throw; we should catch or check for formula cell type and decide how to handle it. Possibly treat formula as text of the formula. We should log if a formula’s value couldn’t be read. But generally, POI gives either cached value or we can choose to evaluate. We’ll likely not evaluate to avoid complexity. So formula cells might be output as their last cached value (which POI provides) or as the formula text if no cached value. We need to ensure consistency: maybe output formula text in markdown code if no normal value is present.
    - **Non-UTF8 characters**: Sometimes Excel might have weird characters. JSON output should be UTF-8 encoded; any such characters will be handled by the JSON library (escaped if needed). We should be careful to use a Writer with correct encoding if writing to file.

- **Logging**:
    - **Levels**: We will implement at least two levels: info (normal) and debug (verbose). Info-level logs (sent to stderr) will include high-level progress and summary: e.g., "Reading file X...", "Sheet1: 5000 cells processed, compressed to 200 entries", "Writing output...". These give the user feedback that the tool is working (important for large files so user knows it hasn’t hung) without overwhelming. They can be suppressed with `--quiet`. Debug-level (enabled by `--verbose`) will include more granular details: e.g., anchor positions found, maybe lists of anchor rows/cols, compression ratios per module (like how many cells removed by anchor, how many keys in dictionary, how many clusters aggregated), and possibly the time taken for each stage. This is useful for development or if the user is curious about the tool’s decisions.
    - **Logging framework**: We can use a simple logging facade. Scala can use slf4j with Logback or even just print to stderr manually. A small library like **scala-logging** could help manage levels. For simplicity, printing to stderr with our own prefixes [INFO], [DEBUG] might suffice.
    - **Error Logging**: Any exceptions caught will be logged at ERROR level. E.g., "ERROR: Failed to parse sheet 'Data': ... exception message ...". We’ll differentiate between user errors (like file not found) which we report clearly and internal errors (like null pointer) which we log with possibly a stack trace in debug mode.

- **Graceful Degradation**: In case part of the pipeline fails but others could continue, we consider what to do. Most failures will likely abort the process (since if we can’t read the file or something fundamental breaks, we can’t produce output). However, consider a scenario: one sheet out of many fails to parse (maybe some corruption in that sheet). We could skip that sheet with a warning and still output others. We’ll implement such that an exception in processing one sheet doesn’t necessarily stop the whole program for remaining sheets. We catch exceptions around each sheet’s processing in the parallel loop. If one fails, we log error for that sheet and continue with others, then at end maybe exit with code indicating partial failure. The combined JSON would then omit the failed sheet or mark it as failed. Alternatively, to keep it simple, if any sheet fails we fail the entire file processing. This can be decided based on use-case. It might be safer to fail whole file to avoid silently missing a sheet. We’ll document that behavior.

- **Testing & Debugging**: We will test the tool on a variety of spreadsheets (small, large, different formats, edge cases like hidden sheets, merged cells, etc.) and use logging to verify each component. The debug logs will help ensure anchors are being picked correctly, etc. We may include an option `--dump-anchors` or so during development to output an intermediate JSON of just the anchor skeleton for inspection, but that might not be needed by end users.

In summary, the tool will be robust: it will validate inputs, handle exceptions gracefully by informing the user of the issue (with actionable messages), and not crash or hang on unusual input. Logging will be in place to trace the processing pipeline and aid in diagnosing any issues or performance problems. With these measures, users can trust the tool to either produce the correct compressed JSON or give a clear reason why it could not, without leaving them guessing.

## Future Extensibility
While the initial implementation focuses on local CLI operation and the specific compression methods from SpreadsheetLLM, we design the system with future enhancements in mind. Here are several areas for potential extension:

- **Cloud/Distributed Execution**: In the future, we might want to deploy this tool as a service or run it on large clusters for bulk processing of spreadsheets. The architecture we outlined (with clear separation of components and stateless processing of each file) lends itself to this. For example:
    - We could wrap the core library in a simple web server (perhaps using Scala HTTP frameworks like Akka HTTP or http4s) to create an API where a user uploads a spreadsheet and gets back the JSON. This would enable use in web applications or automated pipelines. We’d need to handle file uploads and perhaps add authentication if it’s a public service.
    - For big data scenarios, integrating with Apache Spark is possible (e.g., using the library in a Spark job to compress many spreadsheets in parallel across nodes). Since our code is in Scala, it can fit into the Spark ecosystem. We might then output the JSON to a distributed file system or database.
    - Cloud functions: Packaging the tool into an AWS Lambda or Azure Function could allow serverless usage. We’d need to ensure cold start times are okay (the POI library is a few MB, but manageable) and memory is sufficient. This would allow on-demand processing without managing servers, possibly as part of a larger workflow (like an automated data pipeline that compresses spreadsheets for analysis).

- **Integration with LLMs**: The ultimate purpose of SpreadsheetLLM’s encoding is to feed into LLMs for tasks like question answering or analysis ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=Response%20Generation%20The%20query%20and,accurate%20response%20to%20the%20query)). Our tool could be extended to directly interface with LLM APIs or local models:
    - A possible feature: an option `--ask "<question>"` which after compressing the spreadsheet, takes the JSON (or a further processed prompt) and queries an LLM (like GPT-4 or an open-source model) for an answer. This would implement the “Chain of Spreadsheet (CoS)” approach described in the paper, where the first step is table detection and the second is reasoning ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=To%20extend%20the%20applicability%20of,the%20precise%20boundaries%20of%20the)) ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=Response%20Generation%20The%20query%20and,accurate%20response%20to%20the%20query)). We could attempt a simpler version: since our JSON is structured, we might write a prompt that instructs the LLM to find the relevant table or cells for the question and then answer. This is a complex feature (involving prompt engineering and possibly fine-tuning), so it might be a separate project or library. However, by providing the compressed data, we make it much more feasible to use an LLM externally.
    - Alternatively, we can integrate with tools like LangChain or llama-index: those libraries could take structured data (like JSON of a table) and build indices for question answering. We could add functionality to output in a format directly usable by such tools, or even call them internally.
    - Fine-tuning a smaller LLM on this representation could also be an extension. Our Scala library could be part of a pipeline that prepares training data (taking raw spreadsheets, outputting JSON, which is then used to train an LLM to understand the structure).

- **Enhanced Format Support**: Currently, we focus on Excel files. In the future, we might:
    - Support **CSV or TSV** files as input. CSV is simpler (no formatting), but we could still create a JSON with the same structure (each value’s addresses would be like row/col indices). This might broaden the tool’s use beyond Excel to any table data.
    - Support **Google Sheets** via their API (pulling the data and then running compression). This would require network integration and API credentials, which is beyond local-only usage, but as an extension it’s conceivable to allow a Google Sheet URL as input.
    - Support **ODF spreadsheets (.ods)** from LibreOffice. Apache POI doesn’t handle ODS, but there are libraries (maybe ODFToolkit) that can parse ODS. This could be added to the parser module.
    - Handle macros or VBA code: not particularly relevant to data compression, but if needed, we could at least note their presence or extract macro names. Same for Excel charts or images: currently we ignore them, but a future version might want to list them (e.g., "Chart: SalesChart present on sheet" or embed an image placeholder).

- **Refining Compression Techniques**: Future research or practical experience may suggest new compression methods:
    - For example, **semantic compression**: summarizing long text in cells using an LLM (which goes beyond deterministic rules). We could have an option to use an LLM to condense a very large text cell (like a long description) into a shorter summary for the JSON. This hybrid approach mixes deterministic and AI compression.
    - **Adaptive thresholds**: Instead of a fixed *k* for anchors, we could have the tool decide *k* based on sheet size (bigger sheets might tolerate a higher k and still be within token limits). We could implement a heuristic or allow `--auto-anchor` mode that tries different compression levels to target a certain token count budget.
    - **Table Recognition**: If multiple tables exist in one sheet separated by blank rows, our anchor detection might catch them but if not, a more advanced approach could explicitly split the sheet into multiple table regions and compress each separately. This aligns with the boundary detection in the paper ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=Initially%2C%20this%20method%20enumerates%20bounding,layout%20understanding%20of%20a%20spread)) ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=In%20the%20fourth%20step%2C%20overlapping,top%20boundaries%2C%20heuristics%20use%20the)). In the future, we could incorporate a more sophisticated table detection algorithm (maybe even a machine learning model) to identify tables in a sheet. Our current approach is heuristic; replacing or augmenting it with a learned model could improve accuracy in complex cases. The architecture (separating anchor extraction module) makes it possible to swap in a different implementation later.
    - **Formula representation**: We largely ignore formulas in the first version. Future versions might incorporate formula dependency graphs to preserve the relationships between cells. For example, representing that "Cell F2 is the sum of C2:C5" could be valuable for certain analytical tasks. We could output a simplified formula or a reference in JSON (like `"SUM(C2:C5)": "F2"`). This becomes more important if the tool is used for tasks beyond table understanding, like auditing spreadsheet logic.
    - **Metadata**: We might include more spreadsheet metadata in output: sheet names, perhaps cell comments, or data validation rules, which we skip initially. If needed, the JSON schema could be extended to include, say, a "comments" section or "metadata" section with information like "Sheet protected: true/false", "Defined named ranges", etc. These are not core to the content, but in some contexts could be useful (and they don’t cost many tokens).

- **User Interface Improvements**: While the tool is CLI, we might consider creating a simple GUI or VSCode plugin in the future for users less comfortable with command line. The core library can be reused; only a thin UI layer would need to call it. Similarly, integration as an Excel add-in (in C#/VBA calling a compiled DLL of our Scala code via JNI or as a REST service) could let users compress directly from Excel. These are more speculative, but possible if there’s demand.

- **Open Source Collaboration**: If this is an open-source project, we should structure the code for easy contributions. That means good documentation, a clear README (with usage examples like above), and possibly a plugin architecture for new compression modules. For example, one could imagine a plugin that adds a new key compression step (maybe compressing strings by removing vowels or using abbreviations – not in paper, but hypothetically). We could design the Compression Engine to allow custom processors in the pipeline. In Scala, that could be done by traits or a list of strategy objects that transform the model. We already have distinct functions for anchor, index, aggregation; making it extensible means future developers (or ourselves) can insert new steps or modify existing ones easily.

- **Testing and Benchmarks**: Future work will involve adding more unit tests (for each module’s logic) and end-to-end tests on known spreadsheets to ensure correctness. Also, benchmarking on large files to guide performance tuning will be an ongoing effort. We might include a `--benchmark` flag to output timing info, or simply use external profilers.

By keeping these extensibility points in mind, we ensure that the Scala library we implement is not a dead-end but a foundation. The structured plan and modular architecture mean we can iterate on the tool as new research emerges or new use-cases appear, such as integrating directly with AI services or handling new spreadsheet features.

In conclusion, the plan outlined provides a comprehensive blueprint for building the CLI-based Scala library from scratch. We have a high-level architecture with modular components, a clear sequential pipeline of operations, defined data structures for JSON output (with markdown for formatting), detailed algorithms for each compression technique ([Microsoft unveils software that allows LLMs to work with spreadsheets](https://techxplore.com/news/2024-07-microsoft-unveils-software-llms-spreadsheets.html#:~:text=Once%20in%20place%2C%20rows%20and,for%20data%20%2034%20aggregation)) ([2407.09025v1.pdf](file://file-GaiNCjpruBZ2mxBChCwHDR#:~:text=depart%20from%20traditional%20row,usage%20while%20preserving%20data%20integrity)), and considerations for parallelism, performance, and robustness. Following this plan, a developer can implement the repository step by step, testing each module, and achieve a tool that significantly compresses spreadsheet data (on the order of 25x smaller) ([SpreadsheetLLM: Encoding Spreadsheets for Large Language Models - Microsoft Research](https://www.microsoft.com/en-us/research/publication/encoding-spreadsheets-for-large-language-models/#:~:text=spreadsheet%20table%20detection%20task%2C%20outperforming,LLMs%E2%80%99s%20performance%20on%20spreadsheet%20data)) while retaining the critical structure – exactly as inspired by Microsoft’s SpreadsheetLLM research. This will enable practical use of large spreadsheets with LLMs and other text-based analysis tools, opening the door to advanced spreadsheet querying and reasoning in resource-constrained environments.